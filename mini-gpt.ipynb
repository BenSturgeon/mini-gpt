{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2213b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 16 14:15:46 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1060         Off| 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   54C    P8                6W /  N/A|     63MiB /  6144MiB |     29%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       587      G   /usr/lib/Xorg                                61MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c13814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e83d1b8",
   "metadata": {},
   "source": [
    "\n",
    "# Plan of action\n",
    "\n",
    "## Steps\n",
    "* Download the data\n",
    "* Tokenizer\n",
    "* Batch creator\n",
    "* Create a basic forward pass\n",
    "* self attention layer\n",
    "* Create a training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb688ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# download tiny shakespeare\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d2a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the file directly to a variable\n",
    "text = urllib.request.urlopen(url).read().decode('utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a9bf2e3",
   "metadata": {},
   "source": [
    "## Create a tokenizer at the character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4b51312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "[57, 1, 1, 42, 61, 57, 39, 53, 39]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(text))\n",
    "vocab_size = len(tokens)\n",
    "print(vocab_size)\n",
    "\n",
    "# Create an encoder decoder for our tokens to turn them into numbers and back\n",
    "encoder_decoder = {token: i for i, token in enumerate(tokens)}\n",
    "decoder_encoder = {i: token for i, token in enumerate(tokens)}\n",
    "\n",
    "encode = lambda x: [encoder_decoder[i] for i in x]\n",
    "decode = lambda x: \"\".join([decoder_encoder[i] for i in x])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15488944",
   "metadata": {},
   "source": [
    "## Creating our dataset\n",
    "We split the data into training and validation with 90/10 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d8fd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "split_val = int(len(data) * 0.9)\n",
    "train_data = data[:split_val]\n",
    "val_data = data[split_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d34883c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d91be20",
   "metadata": {},
   "source": [
    "### Turning our data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a946a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[39, 19, 19, 42, 61, 11, 42, 18],\n",
      "        [11, 61, 57, 39, 53, 42,  9, 29],\n",
      "        [22, 57,  9, 61, 42, 33,  9,  3],\n",
      "        [57,  9, 25, 25, 42, 18, 39, 42]])\n",
      "\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[19, 19, 42, 61, 11, 42, 18, 39],\n",
      "        [61, 57, 39, 53, 42,  9, 29, 39],\n",
      "        [57,  9, 61, 42, 33,  9,  3, 42],\n",
      "        [ 9, 25, 25, 42, 18, 39, 42, 57]])\n",
      "Input is tensor([39]) and target is 19\n",
      "Input is tensor([39, 19]) and target is 19\n",
      "Input is tensor([39, 19, 19]) and target is 42\n",
      "Input is tensor([39, 19, 19, 42]) and target is 61\n",
      "Input is tensor([39, 19, 19, 42, 61]) and target is 11\n",
      "Input is tensor([39, 19, 19, 42, 61, 11]) and target is 42\n",
      "Input is tensor([39, 19, 19, 42, 61, 11, 42]) and target is 18\n",
      "Input is tensor([39, 19, 19, 42, 61, 11, 42, 18]) and target is 39\n",
      "Input is tensor([11]) and target is 61\n",
      "Input is tensor([11, 61]) and target is 57\n",
      "Input is tensor([11, 61, 57]) and target is 39\n",
      "Input is tensor([11, 61, 57, 39]) and target is 53\n",
      "Input is tensor([11, 61, 57, 39, 53]) and target is 42\n",
      "Input is tensor([11, 61, 57, 39, 53, 42]) and target is 9\n",
      "Input is tensor([11, 61, 57, 39, 53, 42,  9]) and target is 29\n",
      "Input is tensor([11, 61, 57, 39, 53, 42,  9, 29]) and target is 39\n",
      "Input is tensor([22]) and target is 57\n",
      "Input is tensor([22, 57]) and target is 9\n",
      "Input is tensor([22, 57,  9]) and target is 61\n",
      "Input is tensor([22, 57,  9, 61]) and target is 42\n",
      "Input is tensor([22, 57,  9, 61, 42]) and target is 33\n",
      "Input is tensor([22, 57,  9, 61, 42, 33]) and target is 9\n",
      "Input is tensor([22, 57,  9, 61, 42, 33,  9]) and target is 3\n",
      "Input is tensor([22, 57,  9, 61, 42, 33,  9,  3]) and target is 42\n",
      "Input is tensor([57]) and target is 9\n",
      "Input is tensor([57,  9]) and target is 25\n",
      "Input is tensor([57,  9, 25]) and target is 25\n",
      "Input is tensor([57,  9, 25, 25]) and target is 42\n",
      "Input is tensor([57,  9, 25, 25, 42]) and target is 18\n",
      "Input is tensor([57,  9, 25, 25, 42, 18]) and target is 39\n",
      "Input is tensor([57,  9, 25, 25, 42, 18, 39]) and target is 42\n",
      "Input is tensor([57,  9, 25, 25, 42, 18, 39, 42]) and target is 57\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    batch_start_indexes = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in batch_start_indexes])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in batch_start_indexes])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(f\"\"\"\n",
    "inputs:\n",
    "{xb.shape}\n",
    "{xb}\n",
    "\n",
    "targets:\n",
    "{yb.shape}\n",
    "{yb}\"\"\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        x = xb[b][:t+1]\n",
    "        y = yb[b][t]\n",
    "        print(f\"Input is {x} and target is {y}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "394512b0",
   "metadata": {},
   "source": [
    "### Creating our model\n",
    "Our goal is to create a simple bigram model using pytorch nn.Module as our basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3b3f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.7160, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            # Where \n",
    "            # B = batch_size = 4\n",
    "            # T = time = 8\n",
    "            # C = channel = 65 = vocab_size\n",
    "            #  We change the shapes of our logits to get them in the shape needed to use pytorch's cross_entropy function\n",
    "\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, x_input, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(x_input) # we're not using loss, as we're generating\n",
    "\n",
    "            next_token = logits[:, -1,:]\n",
    "\n",
    "            probabilities = F.softmax(next_token, dim=-1)\n",
    "\n",
    "            top_answer = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "            x_input = torch.cat((x_input, top_answer), dim=1) # B, T+1. Appending to 1st dimension which is the time dimension\n",
    "\n",
    "        return x_input\n",
    "        \n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss) # Loss is very high at this point, 4.6 \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uZ:fUv&gDQCFrxScxPT'zxpLJGWrPU. !hpSXxxBeCchElktxnGZv;erHU,LhyYhyLb&tOo-nQLBc'NdbN!hq$DDHzRLP\n",
      "BMilhya\n"
     ]
    }
   ],
   "source": [
    "x_input = torch.zeros((1,1),dtype=torch.long )\n",
    "print(decode(model.generate(x_input = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
