{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2213b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  9 11:56:38 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1060         Off| 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   55C    P8                4W /  N/A|      6MiB /  6144MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       572      G   /usr/lib/Xorg                                 4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c13814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e83d1b8",
   "metadata": {},
   "source": [
    "\n",
    "# Plan of action\n",
    "\n",
    "## Steps\n",
    "* Download the data\n",
    "* Tokenizer\n",
    "* Batch creator\n",
    "* Create a basic forward pass\n",
    "* self attention layer\n",
    "* Create a training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb688ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# download tiny shakespeare\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d2a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the file directly to a variable\n",
    "text = urllib.request.urlopen(url).read().decode('utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a9bf2e3",
   "metadata": {},
   "source": [
    "## Create a tokenizer at the character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4b51312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "[49, 14, 14, 26, 17, 49, 48, 0, 48]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(text))\n",
    "vocab_size = len(tokens)\n",
    "print(vocab_size)\n",
    "\n",
    "# Create an encoder decoder for our tokens to turn them into numbers and back\n",
    "encoder_decoder = {token: i for i, token in enumerate(tokens)}\n",
    "decoder_encoder = {i: token for i, token in enumerate(tokens)}\n",
    "\n",
    "encode = lambda x: [encoder_decoder[i] for i in x]\n",
    "decode = lambda x: \"\".join([decoder_encoder[i] for i in x])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15488944",
   "metadata": {},
   "source": [
    "## Creating our dataset\n",
    "We split the data into training and validation with 90/10 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d8fd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "split_val = int(len(data) * 0.9)\n",
    "train_data = data[:split_val]\n",
    "val_data = data[split_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d34883c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d91be20",
   "metadata": {},
   "source": [
    "### Turning our data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a946a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 26, 41, 63, 25, 21, 24, 26],\n",
      "        [26, 41, 19, 18, 26,  9, 19, 14],\n",
      "        [14, 21, 21, 26, 25, 33, 14, 17],\n",
      "        [48,  0, 21, 63, 63, 42, 26, 17]])\n",
      "\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[26, 41, 63, 25, 21, 24, 26, 49],\n",
      "        [41, 19, 18, 26,  9, 19, 14,  0],\n",
      "        [21, 21, 26, 25, 33, 14, 17, 48],\n",
      "        [ 0, 21, 63, 63, 42, 26, 17, 49]])\n",
      "Input is tensor([17]) and target is 26\n",
      "Input is tensor([17, 26]) and target is 41\n",
      "Input is tensor([17, 26, 41]) and target is 63\n",
      "Input is tensor([17, 26, 41, 63]) and target is 25\n",
      "Input is tensor([17, 26, 41, 63, 25]) and target is 21\n",
      "Input is tensor([17, 26, 41, 63, 25, 21]) and target is 24\n",
      "Input is tensor([17, 26, 41, 63, 25, 21, 24]) and target is 26\n",
      "Input is tensor([17, 26, 41, 63, 25, 21, 24, 26]) and target is 49\n",
      "Input is tensor([26]) and target is 41\n",
      "Input is tensor([26, 41]) and target is 19\n",
      "Input is tensor([26, 41, 19]) and target is 18\n",
      "Input is tensor([26, 41, 19, 18]) and target is 26\n",
      "Input is tensor([26, 41, 19, 18, 26]) and target is 9\n",
      "Input is tensor([26, 41, 19, 18, 26,  9]) and target is 19\n",
      "Input is tensor([26, 41, 19, 18, 26,  9, 19]) and target is 14\n",
      "Input is tensor([26, 41, 19, 18, 26,  9, 19, 14]) and target is 0\n",
      "Input is tensor([14]) and target is 21\n",
      "Input is tensor([14, 21]) and target is 21\n",
      "Input is tensor([14, 21, 21]) and target is 26\n",
      "Input is tensor([14, 21, 21, 26]) and target is 25\n",
      "Input is tensor([14, 21, 21, 26, 25]) and target is 33\n",
      "Input is tensor([14, 21, 21, 26, 25, 33]) and target is 14\n",
      "Input is tensor([14, 21, 21, 26, 25, 33, 14]) and target is 17\n",
      "Input is tensor([14, 21, 21, 26, 25, 33, 14, 17]) and target is 48\n",
      "Input is tensor([48]) and target is 0\n",
      "Input is tensor([48,  0]) and target is 21\n",
      "Input is tensor([48,  0, 21]) and target is 63\n",
      "Input is tensor([48,  0, 21, 63]) and target is 63\n",
      "Input is tensor([48,  0, 21, 63, 63]) and target is 42\n",
      "Input is tensor([48,  0, 21, 63, 63, 42]) and target is 26\n",
      "Input is tensor([48,  0, 21, 63, 63, 42, 26]) and target is 17\n",
      "Input is tensor([48,  0, 21, 63, 63, 42, 26, 17]) and target is 49\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    batch_start_indexes = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in batch_start_indexes])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in batch_start_indexes])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(f\"\"\"\n",
    "inputs:\n",
    "{xb.shape}\n",
    "{xb}\n",
    "\n",
    "targets:\n",
    "{yb.shape}\n",
    "{yb}\"\"\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        x = xb[b][:t+1]\n",
    "        y = yb[b][t]\n",
    "        print(f\"Input is {x} and target is {y}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "394512b0",
   "metadata": {},
   "source": [
    "### Creating our model\n",
    "Our goal is to create a simple bigram model using pytorch nn.Module as our basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b3f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8668, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            # Where \n",
    "            # B = batch_size = 4\n",
    "            # T = time = 8\n",
    "            # C = channel = 65 = vocab_size\n",
    "            #  We change the shapes of our logits to get them in the shape needed to use pytorch's cross_entropy function\n",
    "\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, x_input, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(x_input) # we're not using loss, as we're generating\n",
    "\n",
    "            next_token = logits[:, -1,:]\n",
    "\n",
    "            probabilities = F.softmax(next_token, dim=-1)\n",
    "\n",
    "            top_answer = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "            x_input = torch.cat((x_input, top_answer), dim=1) # B, T+1. Appending to 1st dimension which is the time dimension\n",
    "\n",
    "        return x_input\n",
    "        \n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss) # Loss is very high at this point, 4.6 \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rc$OX!qxDhCmIdNedybKPdFY.WlIyXzk&VFN\n",
      "ddBTCeVwug,dtWc!oTIMXJYVRLVRYsq,p?'thYBeKGHsG&VQ:DDMPEYyAB3UuVRf\n"
     ]
    }
   ],
   "source": [
    "x_input = torch.zeros((1,1),dtype=torch.long )\n",
    "print(decode(model.generate(x_input, max_new_tokens=100)[0].tolist())) \n",
    "# Output is garbage, as we have not begun any training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eff0e553",
   "metadata": {},
   "source": [
    "### Creating our backward pass\n",
    "In this step we create an optimizer and demonstrate a basic gradient descent loop. \n",
    "\n",
    "So far our model is just an embedding table with the dimensions of vocab_size * vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7eeaf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85289c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4971, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for i in range(5000):\n",
    "    xb, yb = get_batch(batch_size)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f8a90bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n",
      "\n",
      "My, ged err\n",
      "Wal m\n",
      "PEZjYounoowZANIO:\n",
      "HINTe aShe athatliacus caryosaJ.\n",
      "Anenysknthace yoFie hbe?-Mave\n"
     ]
    }
   ],
   "source": [
    "x_input = torch.zeros((1,1),dtype=torch.long )\n",
    "print(decode(model.generate(x_input, max_new_tokens=100)[0].tolist())) \n",
    "# Output should look somewhat more sensible, and it does! \n",
    "# This is because the tokens have some idea about what should come next just through information encoded in their own embeddings.\n",
    "# However, we observe a plateau in loss of around 2.3. We'll need to implement new tricks to break through."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "710d344c",
   "metadata": {},
   "source": [
    "### Adding self-attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37da8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[5., 7.],\n",
      "        [2., 0.],\n",
      "        [5., 3.]])\n",
      "--\n",
      "c=\n",
      "tensor([[5.0000, 7.0000],\n",
      "        [3.5000, 3.5000],\n",
      "        [4.0000, 3.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a/torch.sum(a,1,keepdim=True)\n",
    "b = torch.randint(0,10, (3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b58c7f48",
   "metadata": {},
   "source": [
    "The purpose of the following example is to demonstrate the simplest implementation of how tokens can communicate with each other.\n",
    "\n",
    "In this case we just average out all the values of the previous token's channels, which is obviously very lossy, but this is simply illustrative.\n",
    "\n",
    "We will have a way to add all that back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4840e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True]) tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n",
    "\n",
    "\n",
    "# Here we use a bag of words (bow) to illustrate our averaging example\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "# The purpose of this is to show that the rows of xbow are equal to the average of the values in all previous rows of x\n",
    "\n",
    "print(xbow[0][1] == torch.mean(x[0][:2],0), xbow[0][2] == torch.mean(x[0][:3],0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab18a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "print(wei.sum(1, keepdim=True))\n",
    "wei = wei/wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "xbow2 = wei@x\n",
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1852d319",
   "metadata": {},
   "source": [
    "Our next step is to demonstrate that we can do the above using softmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2df0924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros(T,T)\n",
    "print(wei)\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) \n",
    "print(wei)\n",
    "wei = torch.softmax(wei,dim=1)\n",
    "print(wei)\n",
    "xbow3 = wei@x\n",
    "torch.allclose(xbow3,xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee6bff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-inf)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(float('-inf'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcb8938b",
   "metadata": {},
   "source": [
    "To determine the attention of words (more exactly tokens) we use ‘queries’, ‘keys’ and ‘values’.\n",
    "\n",
    "All of them are presented in vectors. \n",
    "\n",
    "Keys activate depending on the strength of closeness with the query vector as determined by dot product.\n",
    "\n",
    "Keys are an encoded representation for values, in simple cases they can be the same. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83a8bab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3116, -0.2300,  0.0999,  0.3821, -0.1887,  0.3470,  0.1903, -0.0801],\n",
      "        [-0.5893, -0.2927,  0.0184,  0.5972, -0.3858,  0.1841, -0.0098,  0.0517],\n",
      "        [-0.1808, -0.2229,  0.0135, -0.0674, -0.1740, -0.2528,  0.0132, -0.1688],\n",
      "        [ 0.1385, -0.1417, -0.0595, -0.1502, -0.0990, -0.2069, -0.2285, -0.1814],\n",
      "        [-0.2221,  0.0033, -0.1393, -0.2334,  0.3600,  0.1527,  0.0657,  0.1637],\n",
      "        [-0.0553,  0.4270, -0.0195, -0.1755,  0.5913, -0.4460,  0.2508,  0.2156],\n",
      "        [ 0.1923,  0.3474, -0.0463, -0.0558,  0.1077,  0.2230, -0.0969,  0.1423],\n",
      "        [-0.3190, -0.0729, -0.1468,  0.1043, -0.1412, -0.1035,  0.1137,  0.1114]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.3116,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.5893, -0.2927,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.1808, -0.2229,  0.0135,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1385, -0.1417, -0.0595, -0.1502,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.2221,  0.0033, -0.1393, -0.2334,  0.3600,    -inf,    -inf,    -inf],\n",
      "        [-0.0553,  0.4270, -0.0195, -0.1755,  0.5913, -0.4460,    -inf,    -inf],\n",
      "        [ 0.1923,  0.3474, -0.0463, -0.0558,  0.1077,  0.2230, -0.0969,    -inf],\n",
      "        [-0.3190, -0.0729, -0.1468,  0.1043, -0.1412, -0.1035,  0.1137,  0.1114]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4264, 0.5736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3151, 0.3022, 0.3827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3007, 0.2272, 0.2467, 0.2253, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1635, 0.2048, 0.1776, 0.1616, 0.2926, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1403, 0.2272, 0.1454, 0.1244, 0.2678, 0.0949, 0.0000, 0.0000],\n",
      "        [0.1554, 0.1815, 0.1224, 0.1213, 0.1428, 0.1603, 0.1164, 0.0000],\n",
      "        [0.0952, 0.1217, 0.1130, 0.1453, 0.1137, 0.1180, 0.1467, 0.1464]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "#Attention head\n",
    "head_size = 16\n",
    "key = nn.Linear(C,head_size,  bias=False)\n",
    "query = nn.Linear(C,head_size, bias=False)\n",
    "value = nn.Linear(C,head_size, bias=False)\n",
    "k = key(x)      # B,T,16\n",
    "q = query(x)    # B,T,16\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "print(wei[0])\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "print(wei[0])\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei[0])\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "print(v.shape)\n",
    "print(out.shape)\n",
    "# print(wei[0])\n",
    "# print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3efd136",
   "metadata": {},
   "source": [
    "### Code explanations of the above:\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "\n",
    "The tranpose is used so we end up with a matrix of B,T,T:\n",
    "\n",
    "(B,T,16) @ (B,16,T) ---> B, T, T: our desired shape\n",
    "\n",
    "\n",
    "This lets us do batch matrix multiplication on our tril matrix which is size(16,16)\n",
    "\n",
    "\n",
    "We apply the normalisation of  C**-0.5 to our wei variable as a normalisation step. We divide by the square route of our head size so that we avoid peaks that are too high in our initial weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54ef1d3b",
   "metadata": {},
   "source": [
    "### Building our multi-head attention blocks\n",
    "We want to build out our heads into parallel layers.\n",
    "\n",
    "This is analogous to group convolutions and has speed advantages (parallelisation) while still giving us the benefits of more trained parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46c35241",
   "metadata": {},
   "source": [
    "### Cleaning our head class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e78c26ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 32\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"A single self-attention head\"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd,head_size,  bias=False)\n",
    "        self.query = nn.Linear(n_embd,head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd,head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        if reverse:\n",
    "\n",
    "            q = self.key(x)      \n",
    "            k = self.query(x)   \n",
    "        else:\n",
    "            k = self.key(x)      \n",
    "            q = self.query(x)   \n",
    "\n",
    "        # To determine the attention of words (more exactly tokens) we use ‘queries’, ‘keys’ and ‘values’.\n",
    "        # All of them are presented in vectors. \n",
    "        # Keys activate depending on the strength of closeness with the query vector as determined by dot product.\n",
    "        # Keys are an encoded representation for values, in simple cases they can be the same. \n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,16) @ (B,16,T) ---> B, T, T: our desired shape\n",
    "\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd5f503c",
   "metadata": {},
   "source": [
    "### Now we have that we can easily create our multi-head attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bee9f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        out = torch.cat([h(x, reverse=reverse) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out)) # Projection back into the residual pathway\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf35112d",
   "metadata": {},
   "source": [
    "Here we use a ModuleList to create a batch of our heads, as well as a projection layer.\n",
    "\n",
    "The module list makes it easy for us to iterate through our heads and pass input through them.\n",
    "\n",
    "The projection allows us to condense the dimensions of our matrices back to the size expected by the residual stream. This allows us to expand our matrices into larger dimensions for inference and then shrink them back down when passing back to the residual stream. This gives us the benefit of doing inference at larger scales.\n",
    "\n",
    "### Explanation on the residual stream\n",
    "\n",
    "The residual stream is the thread going through our entire architecture to which we add the oututs of each of our inference operations.\n",
    "\n",
    "The residual stream is highly beneficial when we are working with deeper neural nets as it allows the backpropogation to propogate to much earlier layers of the network, rather than all the optimisation power being spent on the later layers.\n",
    "\n",
    "This is also sometimes referred to as adding \"skip connections\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "115cf37a",
   "metadata": {},
   "source": [
    "### Adding our feedforward network\n",
    "\n",
    "Finally we add a simple linear feed forward network which gives our network the opportunity to \"think\" about the information contained in each node now that they have had the opportunity to share information via self-attention.\n",
    "\n",
    "The inputs to the feedforward network are simply the results of the dot product of our value embeddings with the outputs of our self-attention queries and keys dot-product operations.\n",
    "\n",
    "In other words:\n",
    "attention_matrix = dot(keys(x), queries(x))\n",
    "\n",
    "attention_matrix = tril_masking_operation(attention_matrix)\n",
    "\n",
    "attention_based_values = dot(attention_matrix, values(x))\n",
    "\n",
    "optimised_outputs = feed_forward(attention_based_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9a1eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4* n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2665bda",
   "metadata": {},
   "source": [
    "### Finally we condense everything into a single block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ebea8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadAttention(n_head, self.head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        ln_x = self.ln1(x)\n",
    "        x = x + self.sa(ln_x, reverse= reverse) # The adding of the values to x is our residual connections, or skip connections\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f43242e8",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "Here we combine everything together. We have you multihead attention which produces outputs for our FeedForward network.\n",
    "\n",
    "To add in our residual stream we simply make the following changes which are present above:\n",
    "\n",
    "x = self.sa(ln_x, reverse= reverse) \n",
    "x = self.ffwd(self.ln2(x))\n",
    "\n",
    "x = x + self.sa(ln_x, reverse= reverse) \n",
    "x = x + self.ffwd(self.ln2(x))\n",
    "\n",
    "By doing this we are keeping our original values which we continuously pass through the network and simply adding to them. This lets our network optimise more efficiently as certain things which can simply be left unchanged in the original input can be left unchanged without the network trying to calculate them again from scratch, which can take up a lot of the parameter's optimisation power."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff044d69",
   "metadata": {},
   "source": [
    "### Adding our layernorm\n",
    "We have yet to discuss normalisation. The residual stream and layernorm are not unique to the transformer. The real innovation that they contributed was the self-attention mechanism.\n",
    "\n",
    "However they are very helpful and necessary for good performance.\n",
    "\n",
    "The layernorm normalisation is essentailly reducing very large variances in our input data to a more manageable range.\n",
    "The goal is make all the values in our input to have a mean of roughly 0 and a standard deviation of 1.\n",
    "\n",
    "We do this by minusing the average of x from x and then dividing that value by the square root of the (variance of x * epsilon).\n",
    "\n",
    "in this case epsilon is a constant we add to avoid situations where we would divide by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2197ccf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "556f7c0f",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now we have assembled all of the pieces, the final step simply involves constucting everything we have built into a single model and training it. In this case we can scale up our model by increasing the values of our n_embd, n_heads, n_layers. \n",
    "\n",
    "The other parameters affect things like our learning rate, and block_size affects how many characters our model looks at in a batch. We also have dropout which is critical but which I will not explain here.\n",
    "\n",
    "For actual running I recommend running the script rather than the model here, but I have included all the necessary code so it should run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ebd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a629180",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 128\n",
    "n_embd = 192\n",
    "n_head =4\n",
    "n_layer = 4\n",
    "lr = 3e-3\n",
    "dropout = 0.2\n",
    "training_iters = 5000\n",
    "eval_interval = 300\n",
    "eval_iters = 200\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "## Helper functions\n",
    "@torch.no_grad() # tells pytorch we don't intend to do backprop. saves memory by not saving gradients.\n",
    "def estimate_loss(model, reverse=False):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for iter in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            logits, loss = model(X,Y, reverse=reverse)\n",
    "            losses[iter] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    batch_start_indexes = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in batch_start_indexes])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in batch_start_indexes])\n",
    "    \n",
    "    x,y = x.to(device), y.to(device)\n",
    "\n",
    "    return x,y\n",
    "\n",
    "def train( model, train_time=1000, output_path=None, save_path=None, num_tokens=250):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for iter in range(train_time):\n",
    "        if iter % eval_interval == 0 and iter>0:\n",
    "            model = model.to(device)\n",
    "            averaged_losses = estimate_loss(model)\n",
    "            reversed_average_loss = estimate_loss(model, reverse=True)\n",
    "            if output_path != None:\n",
    "                with open(output_path, 'a') as f:\n",
    "\n",
    "                    f.write(f\"steps: {iter}  train loss:{averaged_losses['train']:.4f}  test loss:{averaged_losses['val']:.4f} reversed loss: {reversed_average_loss['val']:.4f}\\n\")\n",
    "                    if save_path is not None:\n",
    "                        model.load_state_dict(torch.load(save_path))\n",
    "            else:\n",
    "                print(f\"steps: {iter}  train loss:{averaged_losses['train']:.4f}  test loss:{averaged_losses['val']:.4f} reversed loss: {reversed_average_loss['val']:.4f}\\n\")\n",
    "        \n",
    "        xb, yb = get_batch(batch_size)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks =  nn.ModuleList([Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None, reverse=False):\n",
    "        B,T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x= tok_emb + pos_emb\n",
    "        x = x.to(device)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, reverse=reverse)\n",
    "        x = self.ln(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            # Where \n",
    "            # B = batch_size = 4\n",
    "            # T = time = 8\n",
    "            # C = channel = 65 = vocab_size\n",
    "            #  We change the shapes of our logits to get them in the shape needed to use pytorch's cross_entropy function\n",
    "\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, x_input, max_new_tokens, reverse=False):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            reduced_x_input = x_input[:,-block_size:]\n",
    "            \n",
    "            logits, loss = self.forward(reduced_x_input, reverse=reverse) # we're not using loss, as we're generating\n",
    "\n",
    "            next_token = logits[:, -1,:]\n",
    "\n",
    "            probabilities = F.softmax(next_token, dim=-1)\n",
    "\n",
    "            top_answer = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "            x_input = torch.cat((x_input, top_answer), dim=1) # B, T+1. Appending to 1st dimension which is the time dimension\n",
    "\n",
    "        return x_input\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c51740b",
   "metadata": {},
   "source": [
    "### Training explanation\n",
    "\n",
    "You will notice a value being calculated called \"reversed loss\". This is an experiment which tests if there is a difference in the outputs when the network to produce keys is reversed with the network trained to produce queries in the attention heads. \n",
    "\n",
    "Someone asked me if these networks are in fact learning totally different weights and I created the test to find it. It turns out they are, based on the increasing loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "285225f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 300  train loss:2.2618  test loss:1.8434 reversed loss: 2.6710\n",
      "\n",
      "steps: 600  train loss:2.1127  test loss:1.5700 reversed loss: 2.8561\n",
      "\n",
      "steps: 900  train loss:2.0840  test loss:1.4186 reversed loss: 2.9580\n",
      "\n",
      "steps: 1200  train loss:2.0807  test loss:1.3118 reversed loss: 3.0853\n",
      "\n",
      "steps: 1500  train loss:2.1043  test loss:1.2210 reversed loss: 3.2253\n",
      "\n",
      "steps: 1800  train loss:2.1278  test loss:1.1472 reversed loss: 3.3235\n",
      "\n",
      "steps: 2100  train loss:2.1644  test loss:1.0721 reversed loss: 3.3992\n",
      "\n",
      "steps: 2400  train loss:2.2063  test loss:1.0051 reversed loss: 3.4631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_time = 3000\n",
    "\n",
    "\n",
    "model =train(model, train_time = 3000,  num_tokens=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477f2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
