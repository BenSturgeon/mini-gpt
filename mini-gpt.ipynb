{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2213b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 16 14:15:46 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1060         Off| 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   54C    P8                6W /  N/A|     63MiB /  6144MiB |     29%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       587      G   /usr/lib/Xorg                                61MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "70c13814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e83d1b8",
   "metadata": {},
   "source": [
    "\n",
    "# Plan of action\n",
    "\n",
    "## Steps\n",
    "* Download the data\n",
    "* Tokenizer\n",
    "* Batch creator\n",
    "* Create a basic forward pass\n",
    "* self attention layer\n",
    "* Create a training process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb688ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# download tiny shakespeare\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d2a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the file directly to a variable\n",
    "text = urllib.request.urlopen(url).read().decode('utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a9bf2e3",
   "metadata": {},
   "source": [
    "## Create a tokenizer at the character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4b51312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "[57, 1, 1, 42, 61, 57, 39, 53, 39]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "tokens = list(set(text))\n",
    "vocab_size = len(tokens)\n",
    "print(vocab_size)\n",
    "\n",
    "# Create an encoder decoder for our tokens to turn them into numbers and back\n",
    "encoder_decoder = {token: i for i, token in enumerate(tokens)}\n",
    "decoder_encoder = {i: token for i, token in enumerate(tokens)}\n",
    "\n",
    "encode = lambda x: [encoder_decoder[i] for i in x]\n",
    "decode = lambda x: \"\".join([decoder_encoder[i] for i in x])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15488944",
   "metadata": {},
   "source": [
    "## Creating our dataset\n",
    "We split the data into training and validation with 90/10 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d8fd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "split_val = int(len(data) * 0.9)\n",
    "train_data = data[:split_val]\n",
    "val_data = data[split_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d34883c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d91be20",
   "metadata": {},
   "source": [
    "### Turning our data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a946a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[39, 19, 19, 42, 61, 11, 42, 18],\n",
      "        [11, 61, 57, 39, 53, 42,  9, 29],\n",
      "        [22, 57,  9, 61, 42, 33,  9,  3],\n",
      "        [57,  9, 25, 25, 42, 18, 39, 42]])\n",
      "\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[19, 19, 42, 61, 11, 42, 18, 39],\n",
      "        [61, 57, 39, 53, 42,  9, 29, 39],\n",
      "        [57,  9, 61, 42, 33,  9,  3, 42],\n",
      "        [ 9, 25, 25, 42, 18, 39, 42, 57]])\n",
      "Input is tensor([39]) and target is 19\n",
      "Input is tensor([39, 19]) and target is 19\n",
      "Input is tensor([39, 19, 19]) and target is 42\n",
      "Input is tensor([39, 19, 19, 42]) and target is 61\n",
      "Input is tensor([39, 19, 19, 42, 61]) and target is 11\n",
      "Input is tensor([39, 19, 19, 42, 61, 11]) and target is 42\n",
      "Input is tensor([39, 19, 19, 42, 61, 11, 42]) and target is 18\n",
      "Input is tensor([39, 19, 19, 42, 61, 11, 42, 18]) and target is 39\n",
      "Input is tensor([11]) and target is 61\n",
      "Input is tensor([11, 61]) and target is 57\n",
      "Input is tensor([11, 61, 57]) and target is 39\n",
      "Input is tensor([11, 61, 57, 39]) and target is 53\n",
      "Input is tensor([11, 61, 57, 39, 53]) and target is 42\n",
      "Input is tensor([11, 61, 57, 39, 53, 42]) and target is 9\n",
      "Input is tensor([11, 61, 57, 39, 53, 42,  9]) and target is 29\n",
      "Input is tensor([11, 61, 57, 39, 53, 42,  9, 29]) and target is 39\n",
      "Input is tensor([22]) and target is 57\n",
      "Input is tensor([22, 57]) and target is 9\n",
      "Input is tensor([22, 57,  9]) and target is 61\n",
      "Input is tensor([22, 57,  9, 61]) and target is 42\n",
      "Input is tensor([22, 57,  9, 61, 42]) and target is 33\n",
      "Input is tensor([22, 57,  9, 61, 42, 33]) and target is 9\n",
      "Input is tensor([22, 57,  9, 61, 42, 33,  9]) and target is 3\n",
      "Input is tensor([22, 57,  9, 61, 42, 33,  9,  3]) and target is 42\n",
      "Input is tensor([57]) and target is 9\n",
      "Input is tensor([57,  9]) and target is 25\n",
      "Input is tensor([57,  9, 25]) and target is 25\n",
      "Input is tensor([57,  9, 25, 25]) and target is 42\n",
      "Input is tensor([57,  9, 25, 25, 42]) and target is 18\n",
      "Input is tensor([57,  9, 25, 25, 42, 18]) and target is 39\n",
      "Input is tensor([57,  9, 25, 25, 42, 18, 39]) and target is 42\n",
      "Input is tensor([57,  9, 25, 25, 42, 18, 39, 42]) and target is 57\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    batch_start_indexes = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in batch_start_indexes])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in batch_start_indexes])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(f\"\"\"\n",
    "inputs:\n",
    "{xb.shape}\n",
    "{xb}\n",
    "\n",
    "targets:\n",
    "{yb.shape}\n",
    "{yb}\"\"\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        x = xb[b][:t+1]\n",
    "        y = yb[b][t]\n",
    "        print(f\"Input is {x} and target is {y}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "394512b0",
   "metadata": {},
   "source": [
    "### Creating our model\n",
    "Our goal is to create a simple bigram model using pytorch nn.Module as our basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3b3f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.7160, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            # Where \n",
    "            # B = batch_size = 4\n",
    "            # T = time = 8\n",
    "            # C = channel = 65 = vocab_size\n",
    "            #  We change the shapes of our logits to get them in the shape needed to use pytorch's cross_entropy function\n",
    "\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, x_input, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(x_input) # we're not using loss, as we're generating\n",
    "\n",
    "            next_token = logits[:, -1,:]\n",
    "\n",
    "            probabilities = F.softmax(next_token, dim=-1)\n",
    "\n",
    "            top_answer = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "            x_input = torch.cat((x_input, top_answer), dim=1) # B, T+1. Appending to 1st dimension which is the time dimension\n",
    "\n",
    "        return x_input\n",
    "        \n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss) # Loss is very high at this point, 4.6 \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urang.\n",
      "\n",
      "\n",
      "APEMPEMIOSwetinofeng n m be k ave, hrave\n",
      "ALirer t\n",
      "\n",
      "Heche bed heswames naveand ich\n",
      "Wqut p web\n"
     ]
    }
   ],
   "source": [
    "x_input = torch.zeros((1,1),dtype=torch.long )\n",
    "print(decode(model.generate(x_input, max_new_tokens=100)[0].tolist())) \n",
    "# Output is garbage, as we have not begun any training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eff0e553",
   "metadata": {},
   "source": [
    "### Creating our backward pass\n",
    "In this step we create an optimizer and demonstrate a basic gradient descent loop. \n",
    "\n",
    "So far our model is just an embedding table with the dimensions of vocab_size * vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7eeaf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "85289c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3749, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for i in range(5000):\n",
    "    xb, yb = get_batch(batch_size)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8f8a90bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uco n melamis:\n",
      "HADRI th herit heeseanith, whericiffouas?\n",
      "Hous yo'thalifloul avin memar tweventes:\n",
      "Lur\n"
     ]
    }
   ],
   "source": [
    "x_input = torch.zeros((1,1),dtype=torch.long )\n",
    "print(decode(model.generate(x_input, max_new_tokens=100)[0].tolist())) \n",
    "# Output should look somewhat more sensible, and it does! \n",
    "# This is because the tokens have some idea about what should come next just through information encoded in their own embeddings.\n",
    "# However, we observe a plateau in loss of around 2.3. We'll need to implement new tricks to break through."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "710d344c",
   "metadata": {},
   "source": [
    "### Adding self-attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "37da8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[5., 7.],\n",
      "        [2., 0.],\n",
      "        [5., 3.]])\n",
      "--\n",
      "c=\n",
      "tensor([[5.0000, 7.0000],\n",
      "        [3.5000, 3.5000],\n",
      "        [4.0000, 3.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a/torch.sum(a,1,keepdim=True)\n",
    "b = torch.randint(0,10, (3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b58c7f48",
   "metadata": {},
   "source": [
    "The purpose of the following example is to demonstrate the simplest implementation of how tokens can communicate with each other.\n",
    "\n",
    "In this case we just average out all the values of the previous token's channels, which is obviously very lossy, but this is simply illustrative.\n",
    "\n",
    "We will have a way to add all that back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c4840e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True]) tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n",
    "\n",
    "\n",
    "# Here we use a bag of words (bow) to illustrate our averaging example\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "# The purpose of this is to show that the rows of xbow are equal to the average of the values in all previous rows of x\n",
    "\n",
    "print(xbow[0][1] == torch.mean(x[0][:2],0), xbow[0][2] == torch.mean(x[0][:3],0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ab18a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "print(wei.sum(1, keepdim=True))\n",
    "wei = wei/wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "xbow2 = wei@x\n",
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0924e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
